{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bdd712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2677\n",
      "현재 작품: 나 혼자만 레벨업\n",
      "현재 작품: 접근 불가 레이디\n",
      "현재 작품: 제목을 찾을 수 없음\n",
      "현재 작품: 무협지 악녀인데 내가 제일 쎄!\n",
      "현재 작품: 그녀와 야수\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# CSV 파일에서 titleId 값을 가져오기\n",
    "title_df = pd.read_csv(\"kakao_listly.csv\")  # kakao_listly.csv 파일을 불러오기\n",
    "id_list = title_df['url'].astype(str).tolist()  # url 칼럼의 값들을 문자열로 변환하여 리스트로 저장\n",
    "print(len(id_list))\n",
    "\n",
    "# WebDriver 설정\n",
    "options = Options()\n",
    "options.add_argument('headless')  # 헤드리스 모드\n",
    "options.add_argument('window-size=1920x1080')  # 윈도우 크기 설정\n",
    "options.add_argument(\"disable-gpu\")  # GPU 비활성화\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# 데이터 저장 리스트\n",
    "data = []\n",
    "\n",
    "# URL 순회하며 정보 수집\n",
    "for url in id_list[:5]:  # 예시로 10개만 크롤링\n",
    "    # 오버뷰 페이지 URL 생성\n",
    "    overview_url = f\"{url}?tab_type=overview\"\n",
    "    driver.get(overview_url)\n",
    "    time.sleep(2)  # 페이지가 완전히 로드될 시간을 줍니다.\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    # 제목\n",
    "    title_h2 = soup.find(\"span\", class_=\"font-large3-bold mb-3pxr text-ellipsis break-all text-el-70 line-clamp-2\")\n",
    "    title_text = title_h2.get_text() if title_h2 else \"제목을 찾을 수 없음\"\n",
    "\n",
    "    # 작가\n",
    "    writer_span = soup.find(\"span\", class_=\"font-small2 mb-6pxr text-ellipsis text-el-70 opacity-70 break-word-anywhere line-clamp-2\")\n",
    "    writer_text = writer_span.get_text() if writer_span else \"작가를 찾을 수 없음\"\n",
    "\n",
    "    # 장르 (웹툰 제외하고 두 번째 텍스트만 추출)\n",
    "    genre_spans = soup.find_all(\"span\", class_=\"break-all align-middle\")\n",
    "    genres = [genre.get_text() for genre in genre_spans]\n",
    "    \n",
    "    # 웹툰은 공통이므로 첫 번째 항목은 제외하고 두 번째 장르만 추출\n",
    "    if len(genres) > 1:\n",
    "        genre_text = genres[1]  # 두 번째 장르만 추출\n",
    "    else:\n",
    "        genre_text = \"장르를 찾을 수 없음\"\n",
    "\n",
    "    # 조회수\n",
    "    view_count_span = soup.find(\"span\", class_=\"text-el-70 opacity-70\")\n",
    "    view_count_text = view_count_span.get_text() if view_count_span else \"조회수를 찾을 수 없음\"\n",
    "\n",
    "    # 별점\n",
    "    rating_span = soup.find(\"span\", class_=\"text-el-70 opacity-70\")\n",
    "    rating_text = rating_span.get_text() if rating_span else \"별점을 찾을 수 없음\"\n",
    "\n",
    "    # 회차수\n",
    "    episode_count_span = soup.find(\"span\", class_=\"text-ellipsis break-all line-clamp-1 font-small2-bold text-el-70\")\n",
    "    episode_count_text = episode_count_span.get_text() if episode_count_span else \"회차수를 찾을 수 없음\"\n",
    "\n",
    "    # 댓글수\n",
    "    comment_count_span = soup.find(\"span\", class_=\"text-ellipsis break-all line-clamp-1 font-small2-bold text-el-70\")\n",
    "    comment_count_text = comment_count_span.get_text() if comment_count_span else \"댓글수를 찾을 수 없음\"\n",
    "\n",
    "    # 저장\n",
    "    data.append({\n",
    "        'Title': title_text,\n",
    "        'Writer': writer_text,\n",
    "        'Genre': genre_text,\n",
    "        'View Count': view_count_text,\n",
    "        'Rating': rating_text,\n",
    "        'Episode Count': episode_count_text,\n",
    "        'Comment Count': comment_count_text\n",
    "    })\n",
    "\n",
    "    # 10개 확인용\n",
    "    print(f\"현재 작품: {title_text}\")\n",
    "    time.sleep(3)  # 데이터 확인을 위한 시간\n",
    "\n",
    "# 드라이버 종료\n",
    "driver.quit()\n",
    "\n",
    "# DataFrame 생성 및 출력\n",
    "df = pd.DataFrame(data)[['Title', 'Writer', 'Genre', 'View Count', 'Rating', 'Episode Count', 'Comment Count']]\n",
    "\n",
    "# CSV로 저장\n",
    "df.to_csv('kakao_overview_crawling.csv', index=False, encoding='utf-8-sig')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
